{
  "title": "Delete Duplicate Folders in System",
  "problem_id": "2079",
  "frontend_id": "1948",
  "difficulty": "Hard",
  "problem_slug": "delete-duplicate-folders-in-system",
  "topics": [
    "Array",
    "Hash Table",
    "String",
    "Trie",
    "Hash Function"
  ],
  "description": "Due to a bug, there are many duplicate folders in a file system. You are given a 2D array paths, where paths[i] is an array representing an absolute path to the ith folder in the file system.\nTwo folders (not necessarily on the same level) are identical if they contain the same non-empty set of identical subfolders and underlying subfolder structure. The folders do not need to be at the root level to be identical. If two or more folders are identical, then mark the folders as well as all their subfolders.\nOnce all the identical folders and their subfolders have been marked, the file system will delete all of them. The file system only runs the deletion once, so any folders that become identical after the initial deletion are not deleted.\nReturn the 2D array ans containing the paths of the remaining folders after deleting all the marked folders. The paths may be returned in any order.\nExample 1:\nExample 2:\nExample 3:\nConstraints:",
  "examples": [
    {
      "example_num": 1,
      "example_text": "Input: paths = [[\"a\"],[\"c\"],[\"d\"],[\"a\",\"b\"],[\"c\",\"b\"],[\"d\",\"a\"]]\nOutput: [[\"d\"],[\"d\",\"a\"]]\nExplanation: The file structure is as shown.\nFolders \"/a\" and \"/c\" (and their subfolders) are marked for deletion because they both contain an empty\nfolder named \"b\".",
      "images": [
        "https://assets.leetcode.com/uploads/2021/07/19/lc-dupfolder1.jpg"
      ]
    },
    {
      "example_num": 2,
      "example_text": "Input: paths = [[\"a\"],[\"c\"],[\"a\",\"b\"],[\"c\",\"b\"],[\"a\",\"b\",\"x\"],[\"a\",\"b\",\"x\",\"y\"],[\"w\"],[\"w\",\"y\"]]\nOutput: [[\"c\"],[\"c\",\"b\"],[\"a\"],[\"a\",\"b\"]]\nExplanation: The file structure is as shown. \nFolders \"/a/b/x\" and \"/w\" (and their subfolders) are marked for deletion because they both contain an empty folder named \"y\".\nNote that folders \"/a\" and \"/c\" are identical after the deletion, but they are not deleted because they were not marked beforehand.",
      "images": [
        "https://assets.leetcode.com/uploads/2021/07/19/lc-dupfolder2.jpg"
      ]
    },
    {
      "example_num": 3,
      "example_text": "Input: paths = [[\"a\",\"b\"],[\"c\",\"d\"],[\"c\"],[\"a\"]]\nOutput: [[\"c\"],[\"c\",\"d\"],[\"a\"],[\"a\",\"b\"]]\nExplanation: All folders are unique in the file system.\nNote that the returned array can be in a different order as the order does not matter.",
      "images": [
        "https://assets.leetcode.com/uploads/2021/07/19/lc-dupfolder3.jpg"
      ]
    }
  ],
  "constraints": [
    "1 <= paths.length <= 2 * 104",
    "1 <= paths[i].length <= 500",
    "1 <= paths[i][j].length <= 10",
    "1 <= sum(paths[i][j].length) <= 2 * 105",
    "path[i][j] consists of lowercase English letters.",
    "No two paths lead to the same folder.",
    "For any folder not at the root level, its parent folder will also be in the input."
  ],
  "follow_ups": [],
  "hints": [
    "Can we use a trie to build the folder structure?",
    "Can we utilize hashing to hash the folder structures?"
  ],
  "code_snippets": {
    "cpp": "class Solution {\npublic:\n    vector<vector<string>> deleteDuplicateFolder(vector<vector<string>>& paths) {\n        \n    }\n};",
    "java": "class Solution {\n    public List<List<String>> deleteDuplicateFolder(List<List<String>> paths) {\n        \n    }\n}",
    "python": "class Solution(object):\n    def deleteDuplicateFolder(self, paths):\n        \"\"\"\n        :type paths: List[List[str]]\n        :rtype: List[List[str]]\n        \"\"\"\n        ",
    "python3": "class Solution:\n    def deleteDuplicateFolder(self, paths: List[List[str]]) -> List[List[str]]:\n        ",
    "c": "/**\n * Return an array of arrays of size *returnSize.\n * The sizes of the arrays are returned as *returnColumnSizes array.\n * Note: Both returned array and *columnSizes array must be malloced, assume caller calls free().\n */\nchar*** deleteDuplicateFolder(char*** paths, int pathsSize, int* pathsColSize, int* returnSize, int** returnColumnSizes) {\n    \n}",
    "csharp": "public class Solution {\n    public IList<IList<string>> DeleteDuplicateFolder(IList<IList<string>> paths) {\n        \n    }\n}",
    "javascript": "/**\n * @param {string[][]} paths\n * @return {string[][]}\n */\nvar deleteDuplicateFolder = function(paths) {\n    \n};",
    "typescript": "function deleteDuplicateFolder(paths: string[][]): string[][] {\n    \n};",
    "php": "class Solution {\n\n    /**\n     * @param String[][] $paths\n     * @return String[][]\n     */\n    function deleteDuplicateFolder($paths) {\n        \n    }\n}",
    "swift": "class Solution {\n    func deleteDuplicateFolder(_ paths: [[String]]) -> [[String]] {\n        \n    }\n}",
    "kotlin": "class Solution {\n    fun deleteDuplicateFolder(paths: List<List<String>>): List<List<String>> {\n        \n    }\n}",
    "dart": "class Solution {\n  List<List<String>> deleteDuplicateFolder(List<List<String>> paths) {\n    \n  }\n}",
    "golang": "func deleteDuplicateFolder(paths [][]string) [][]string {\n    \n}",
    "ruby": "# @param {String[][]} paths\n# @return {String[][]}\ndef delete_duplicate_folder(paths)\n    \nend",
    "scala": "object Solution {\n    def deleteDuplicateFolder(paths: List[List[String]]): List[List[String]] = {\n        \n    }\n}",
    "rust": "impl Solution {\n    pub fn delete_duplicate_folder(paths: Vec<Vec<String>>) -> Vec<Vec<String>> {\n        \n    }\n}",
    "racket": "(define/contract (delete-duplicate-folder paths)\n  (-> (listof (listof string?)) (listof (listof string?)))\n  )",
    "erlang": "-spec delete_duplicate_folder(Paths :: [[unicode:unicode_binary()]]) -> [[unicode:unicode_binary()]].\ndelete_duplicate_folder(Paths) ->\n  .",
    "elixir": "defmodule Solution do\n  @spec delete_duplicate_folder(paths :: [[String.t]]) :: [[String.t]]\n  def delete_duplicate_folder(paths) do\n    \n  end\nend"
  },
  "solution": "### Approach: Serialization-Based Representation of Subtrees\n\n#### Intuition\n\nWe can approach this problem at an abstract level (without worrying about implementation details at first) in three main steps:\n\n1. Build a tree representation of the file system using the input $\\textit{paths}$. This tree is a multi-way tree rooted at /, where each non-root node represents a folder.\n\n2. Traverse the tree starting from the root. As stated in the problem, if two nodes $x$ and $y$ contain subfolders with the same structure (i.e., the same nested arrangement of subfolders, recursively), then both $x$ and $y$ must be deleted. Therefore, to determine the structure of a node’s subtree, we must first traverse all its children and then backtrack to process the node itself. This corresponds to a post-order traversal of a multi-way tree.\n\nWhile backtracking to a node, we serialize its structure and store it in a data structure for later comparison with other nodes.\n\n3. Traverse the tree again from the root. When visiting a node $x$, if its serialized structure appears more than once in the data structure, it means a duplicate exists, and we delete $x$ (i.e., skip it). Otherwise, $x$ is unique, and we record the path from the root to $x$ in the final answer, then recursively visit its children.\n\nAfter this second traversal, all duplicate folders will have been removed, and we will have collected the remaining unique folder paths.\n\n#### Algorithm\n\nLet’s now solve these three steps one by one:\n\n**Step 1: Build the Tree**\n\nWe define a class to represent the nodes of the tree. We create a root node, and for each path in $\\textit{paths}$, we insert its folders into the tree. If you're familiar with the Trie data structure, this step will feel familiar.\n\n**Step 2: Serialize and Identify Duplicates**\n\nThe challenge here is not the post-order traversal itself, but rather how to represent the \"structure\" of a node in a way that can be used to compare nodes.\n\nTo do this, we adopt a serialization approach similar to what is used in [\"297. Serialize and Deserialize Binary Tree\"](https://leetcode.com/problems/serialize-and-deserialize-binary-tree/). Let $\\text{serial}(x)$ represent the serialized structure of node $x$. We define it as follows:\n \n- If $x$ is a leaf node (i.e., has no children), then $\\text{serial}(x)$ is an empty string `\"\"`. For instance, in Example 1, the three leaf nodes `b`, `b`, and `a` all serialize to `\"\"`.\n\n- If $x$ has children $y_1, y_2, \\dots, y_k$, then:\n\n    $$\n    \\text{serial}(x) = y_1(\\text{serial}(y_1))y_2(\\text{serial}(y_2))\\cdots y_k(\\text{serial}(y_k))\n    $$\n\n    In words, we recursively serialize each child, attach its folder name in front of its serialization, and wrap its structure in parentheses. The result is a string representing the structure of $x$'s subtree.\n    \n    However, this naive approach can be order-sensitive. If $x_1$ and $x_2$ have the same children but in different orders, their serializations will differ, even though their structures are equivalent. To handle this, we sort the serialized representations of children before concatenating. This ensures consistent serialization for equivalent subtrees.\n    \nAfter a single post-order traversal of the tree, we can record all serializations in a hash map, where the key is the serialization and the value is its frequency.\n\n**Step 3: Collect Unique Paths**\n\nWe now perform another DFS traversal from the root. We maintain a list path that tracks the current folder path. At each node:\n\n- If the node’s serialization appears more than once in the map, it is a duplicate and should be skipped.\n- Otherwise, the path to this node is added to the final answer, and we recursively traverse its children.\n\n#### Implementation\n\n> The C++ code below builds the tree, serializes it using post-order traversal, and collects the unique folder paths. Note: this version does **not free memory** after execution; in an interview, you may ask whether tree destruction is required.#### Complexity analysis\n\nWe focus here on the time required to compute the serialization of all node structures and the space required to store them in a hash map. All other operations, whether time- or space-related, are asymptotically smaller and can therefore be ignored.\n\nIn the worst case, each node in the tree has a unique serialized structure. Thus, the time and space complexities are both proportional to the total length of all serialized strings. Our task is to find an upper bound on this total length.\n\nTo do this, we use an important and intuitive result from tree theory:\n\n> Let $T$ be an unordered rooted tree. For a node $x$ in $T$, define:\n> - $\\textit{dist}[x]$: the number of nodes on the path from the root to $x$\n> - $\\textit{size}[x]$: the size of the subtree rooted at $x$\n> Then:\n>\n> $$\n> \\sum_{x \\in T} \\textit{dist}[x] = \\sum_{x \\in T} \\textit{size}[x]\n> $$\n\n**Why this holds:**\nFor any node $x'$, it contributes to the subtree size of each of its ancestors (including itself). Therefore, $x'$ appears once in the subtree size of every node along its path from the root. So the total number of appearances of all nodes across all subtree sizes equals the sum of all distances from the root.\n\nNow, returning to our problem:\n\n- The input array paths encodes the full paths from the root to each folder, and the total number of characters across all paths is bounded by $2 \\times 10^5$.\n\n- So $\\sum \\textit{dist}[x] \\leq 2 \\times 10^5$\n\n- And therefore $\\sum \\textit{size}[x] \\leq 2 \\times 10^5$\n\nFor each node $x$, the length of its serialized string representation includes two components:\n\n1. The sum of the lengths of the folder names of all its subfolders. Each subfolder name can have at most 10 characters, so this part is bounded by $10 \\cdot \\textit{size}[x]$.\n\n2. The number of parentheses used for structural disambiguation. Each subfolder is wrapped in a pair of parentheses, contributing at most $2 \\cdot \\textit{size}[x]$ characters.\n\nThus, for any node $x$, the total serialization length is at most:\n\n$$\n12 \\cdot \\textit{size}[x]\n$$\n\nSo the total length of all serialized strings across the entire tree is:\n\n$$\n12 \\cdot \\sum_{x \\in T} \\textit{size}[x] \\leq 12 \\cdot 2 \\times 10^5 = 2.4 \\times 10^6\n$$\n\nHence, the space complexity is $\\mathcal{O}(10^6)$ in the worst case.\n\nAs for the time complexity, even if we account for sorting the child structures (which adds an extra $\\log$ factor), the total operations remain bounded by around $10^7$, which comfortably fits within the time limits.\n\nIt's worth noting that this upper-bound analysis assumes extremely pessimistic and adversarial cases. In practical scenarios, the actual runtime is significantly lower, and this method performs very efficiently.\n\n---"
}