{
  "title": "Check If N and Its Double Exist",
  "problem_id": "1468",
  "frontend_id": "1346",
  "difficulty": "Easy",
  "problem_slug": "check-if-n-and-its-double-exist",
  "topics": [
    "Array",
    "Hash Table",
    "Two Pointers",
    "Binary Search",
    "Sorting"
  ],
  "description": "Given an array arr of integers, check if there exist two indices i and j such that :\nExample 1:\nExample 2:\nConstraints:",
  "examples": [
    {
      "example_num": 1,
      "example_text": "Input: arr = [10,2,5,3]\nOutput: true\nExplanation: For i = 0 and j = 2, arr[i] == 10 == 2 * 5 == 2 * arr[j]",
      "images": []
    },
    {
      "example_num": 2,
      "example_text": "Input: arr = [3,1,7,11]\nOutput: false\nExplanation: There is no i and j that satisfy the conditions.",
      "images": []
    }
  ],
  "constraints": [
    "2 <= arr.length <= 500",
    "-103 <= arr[i] <= 103"
  ],
  "follow_ups": [],
  "hints": [
    "Loop from i = 0 to arr.length, maintaining in a hashTable the array elements from [0, i - 1].",
    "On each step of the loop check if we have seen the element <code>2 * arr[i]</code> so far.",
    "Also check if we have seen <code>arr[i] / 2</code> in case <code>arr[i] % 2 == 0</code>."
  ],
  "code_snippets": {
    "cpp": "class Solution {\npublic:\n    bool checkIfExist(vector<int>& arr) {\n        \n    }\n};",
    "java": "class Solution {\n    public boolean checkIfExist(int[] arr) {\n        \n    }\n}",
    "python": "class Solution(object):\n    def checkIfExist(self, arr):\n        \"\"\"\n        :type arr: List[int]\n        :rtype: bool\n        \"\"\"\n        ",
    "python3": "class Solution:\n    def checkIfExist(self, arr: List[int]) -> bool:\n        ",
    "c": "bool checkIfExist(int* arr, int arrSize) {\n    \n}",
    "csharp": "public class Solution {\n    public bool CheckIfExist(int[] arr) {\n        \n    }\n}",
    "javascript": "/**\n * @param {number[]} arr\n * @return {boolean}\n */\nvar checkIfExist = function(arr) {\n    \n};",
    "typescript": "function checkIfExist(arr: number[]): boolean {\n    \n};",
    "php": "class Solution {\n\n    /**\n     * @param Integer[] $arr\n     * @return Boolean\n     */\n    function checkIfExist($arr) {\n        \n    }\n}",
    "swift": "class Solution {\n    func checkIfExist(_ arr: [Int]) -> Bool {\n        \n    }\n}",
    "kotlin": "class Solution {\n    fun checkIfExist(arr: IntArray): Boolean {\n        \n    }\n}",
    "dart": "class Solution {\n  bool checkIfExist(List<int> arr) {\n    \n  }\n}",
    "golang": "func checkIfExist(arr []int) bool {\n    \n}",
    "ruby": "# @param {Integer[]} arr\n# @return {Boolean}\ndef check_if_exist(arr)\n    \nend",
    "scala": "object Solution {\n    def checkIfExist(arr: Array[Int]): Boolean = {\n        \n    }\n}",
    "rust": "impl Solution {\n    pub fn check_if_exist(arr: Vec<i32>) -> bool {\n        \n    }\n}",
    "racket": "(define/contract (check-if-exist arr)\n  (-> (listof exact-integer?) boolean?)\n  )",
    "erlang": "-spec check_if_exist(Arr :: [integer()]) -> boolean().\ncheck_if_exist(Arr) ->\n  .",
    "elixir": "defmodule Solution do\n  @spec check_if_exist(arr :: [integer]) :: boolean\n  def check_if_exist(arr) do\n    \n  end\nend"
  },
  "solution": "[TOC]\n\n## Solution\n\n--- \n\n### Overview\n\nThis problem is designed to help beginners get comfortable with basic array operations and the implementation of basic data structures. We are given an array and our goal is to find out if there are two different indices, represented by $i$ and $j$, where the value of one is twice the value of the other. \n\nHow to interpret the three conditions:\n- `i` and `j` must be different indices. This might seem like an obvious point unless you consider that $0 = 2 \\times 0$. Without this condition, we'd be able to count just one `0` in the array as satisfying the goal. With this condition, an array would need two `0`s to satisfy the conditions. \n- `0 <= i, j < arr.length` just means that the indices are within the bounds of the array. This condition doesn't mean much, it's essentially a requirement for any array-based algorithm. \n- `arr[i] == 2 * arr[j]` is how we know `i` needs to be double `j`. \n\n---\n\n### Approach 1: Brute Force\n\n#### Intuition\n\nOne simple approach is to calculate the double of each number and then check if that value is in the array. This brute force method directly explores all possible pairs for each element until the result is found. This approach works but is not the most efficient. \n\n#### Algorithm\n\n- Iterate through all pairs of indices `i` and `j` in the array `arr`.\n  - For each pair, check if:\n    - `i != j` (to ensure we aren't comparing the same element).\n    - `arr[i] == 2 * arr[j]` (one element is double the other).\n  - If both conditions are met, return `true` to indicate a valid pair is found.\n\n- If no valid pair is found after checking all pairs, return `false`.\n\n#### Implementation#### Complexity Analysis\n\nLet `n` be the size of the input array `arr`.\n\n- Time complexity: $O(n^2)$\n\n    The algorithm consists of two nested loops. The outer loop iterates over each element in the array `arr`, and the inner loop also iterates over all elements of `arr`. For each pair of indices $i$ and $j$, the algorithm checks the condition $arr[i] == 2 \\times arr[j]$ and ensures $i \\neq j$.\n\n    Since both loops iterate `n` times, the time complexity of the nested loops is $O(n \\times n) = O(n^2)$.\n\n- Space complexity: $O(1)$\n\n    The algorithm does not use any additional data structures that grow with the size of the input. The space used by the algorithm is constant, as it only requires a few variables for the loop indices and condition checking. Therefore, the space complexity is $O(1)$.\n\n---\n\n### Approach 2: Set Lookup\n\n#### Intuition\n\nThe brute force method doesnâ€™t keep track of what itâ€™s seen, so it wastes effort by revisiting the same values each time it loops through the array. Let's break down a solution where we can \"store what weâ€™ve seen\" to speed up lookups.\n\nA hash set works well here because it allows constant-time insertion and lookup. Instead of scanning the array multiple times, we use the set to check if $2 \\times arr[i]$ or $arr[i] / 2$ (when divisible) already exists. If neither condition is met, we add $arr[i]$ to the set and continue. \n\nThis way, we only iterate through the array once and eliminate the unnecessary comparisons we made in the brute force approach.\n \n#### Algorithm\n\n- Initialize an empty set named `seen` to store numbers encountered so far.\n\n- For each `num` in the array `arr`:\n  - Check if `2 * num` or `num / 2` exists in the `seen` set:\n    - If `2 * num` is found in the set, or if `num` is divisible by 2 and `num / 2` is found in the set, return `true` (a valid pair is found).\n  - Add the current number `num` to the `seen` set for future checks.\n\n- If no valid pair is found after checking all elements, return `false`.\n\n#### Implementation#### Complexity Analysis\n\nLet `n` be the size of the input array `arr`.\n\n- Time complexity: $O(n)$\n\n    The algorithm iterates through the array `arr` once, processing each element individually. For each element, it performs two operations:\n    1. Checking if $2 \\times \\text{num}$ or $\\text{num} / 2$ is in the set, which takes $O(1)$ on average due to the constant-time lookup in the set.\n    2. Adding the current element to the set, which also takes $O(1)$ on average for each insertion.\n\n    Since both operations inside the loop take constant time, and the loop runs `n` times, the overall time complexity of the algorithm is $O(n)$.\n\n- Space complexity: $O(n)$\n\n    The primary space usage comes from the set, which stores up to `n` unique elements from the array `arr`. In the worst case, when all elements are unique, the set will contain `n` elements, resulting in a space complexity of $O(n)$.\n\n    No additional significant data structures are used, so the auxiliary space complexity is $O(1)$. Therefore, the total space complexity is $O(n)$.\n \n---\n\n### Approach 3: Sorting + Binary Search\n\n#### Intuition\n\nLet's make a simple observation: except when the values of $i$ and $j$ are both 0, $j$ will always be greater than $i$. Since the relationship between the two is directional, we can sort the array in ascending order and apply one of the most fundamental search algorithms: binary search. \n\nBinary search is a two-pointer technique for efficiently locating a value in an ordered collection. Unlike Approach One, which checks each element individually, binary search repeatedly divides the search range in half which significantly reduces the number of comparisons we need to make.\n\n> For a more comprehensive understanding of binary search, check out the [Binary Search Explore Card ðŸ”—](https://leetcode.com/explore/learn/card/binary-search/). This resource offers an in-depth look at binary search, explaining its key concepts and applications with a variety of problems to solidify understanding of the pattern.\n\nWe start by setting one pointer to the first possible element and the second pointer to the last. At each step, we compare the target value to the middle element of the current range. If the target is greater than the midpoint, we eliminate all elements before the midpoint by moving the first pointer to the position just right of it. If the target is smaller, we eliminate all elements after the midpoint by moving the second pointer to the position just left of it. This process continues until the target is found or the range is empty.\n\n!?!../Documents/1346/1346_approach3.json:805,545!?!#### Algorithm\n\n- Sort the array `arr` in ascending order to enable efficient searching.\n  \n- For each element `arr[i]` in the array:\n  - Calculate the target value as `2 * arr[i]` (double the current number).\n  - Perform a custom binary search for the target in the array:\n    - In the `customBinarySearch` function:\n      - Set `left` to 0 and `right` to `arr.length - 1` to define the search range.\n      - While `left <= right`, calculate the midpoint `mid`.\n        - If `arr[mid] == target`, return the index `mid` (target found).\n        - If `arr[mid] < target`, move the `left` pointer to `mid + 1` to search the right half.\n        - If `arr[mid] > target`, move the `right` pointer to `mid - 1` to search the left half.\n      - If the target is not found, return `-1`.\n  \n  - If the target exists and its index is not the same as the current index `i`, return `true` (found a pair where one element is double the other).\n\n- If no valid pair is found after iterating through the array, return `false`.\n\n#### Implementation#### Complexity Analysis\n\nLet `n` be the size of the input array `arr`.\n\n- Time complexity: $O(n \\log n)$\n\n   The sort function sorts the array in $O(n \\log n)$ time. Sorting is the most time-consuming operation here.\n\n   The for loop iterates through each element in the array, and for each element, it calls the `customBinarySearch` function. The binary search operation itself takes $O(\\log n)$ time, as it divides the search space in half at each step.  \n   \n   Therefore, the time complexity of the loop is $O(n)$ for iterating through the array, and for each iteration, the binary search takes $O(\\log n)$. Thus, the total time complexity for the loop is $O(n \\log n)$.\n\n   Combining both parts, the overall time complexity is $O(n \\log n)$.\n\n- Space complexity: $O(n)$ or $O(\\log n)$\n\n    The space taken by the sorting algorithm depends on the language of implementation:\n\n    In Java, `Arrays.sort()` is implemented using a variant of the Quick Sort algorithm which has a space complexity of $O( \\log n)$.\n    \n    In C++, the `sort()` function is implemented as a hybrid of Quick Sort, Heap Sort, and Insertion Sort, with a worst-case space complexity of $O(\\log n)$.\n    \n    In Python, the `sort()` method sorts a list using the Timsort algorithm which is a combination of Merge Sort and Insertion Sort and has a space complexity of $O(n)$.\n \n   The binary search uses constant space for variables like `left`, `right`, and `mid`. The loop also does not use any additional space other than a few variables. Therefore, the space used by the loop is $O(1)$.\n\n---\n\n### Approach 4: Frequency Hash Map\t\n\n#### Intuition\n\nInstead of using a hash set to find the pair like we did in Approach 2, we can use a frequency map. Some may understandably wonder why we don't use a frequency array instead of a map, which could be more memory-efficient and an excellent choice if the constraint were `>= 0`. However, since this problem allows negative numbers, a frequency map is the better choice. \n\nFirst, we will count the number of occurrences of each number and store their counts in their respective indices. Then, we will iterate again and check each element:\n\n1. If `num` is in the array, we check if `2 * num` also exists using the map.\n2. If `num = 0`, we ensure its count is at least 2 to satisfy `i â‰  j`.\n\n#### Algorithm\n\n- Initialize an empty hash map called `map` to store the count of occurrences of each number in the array.\n\n- For each number `num` in `arr`:\n  - Update the map with the count of `num` by incrementing the value associated with `num` in `map`.\n\n- After populating the map, check for the condition where a number has a double in the array:\n  - For each `num` in `arr`, if `num` is not zero and `map` contains `2 * num`, return `true` (found a number with its double).\n  - If `num` is zero and there are more than one zero in the array (i.e., `map.get(num) > 1`), return `true` (special case where 0 is double of 0).\n\n- If no such pair is found, return `false` (no number has its double in the array).\n\n#### Implementation#### Complexity Analysis\n\nLet `n` be the size of the input array `arr`.\n\n- Time complexity: $O(n)$\n\n    The algorithm consists of two main loops. The first loop iterates through the array `arr` and inserts or updates each element in the hash map. The insertion operation in the hash map takes $O(1)$ on average, so the time complexity of this loop is $O(n)$.\n\n    The second loop also iterates through the array `arr` and performs constant-time operations for each element, including lookups in the hash map (which are $O(1)$ on average). Therefore, the time complexity of this loop is also $O(n)$.\n\n    As a result, the overall time complexity is $O(n) + O(n) = O(n)$.\n\n- Space complexity: $O(n)$\n\n    The space complexity is dominated by the hash map, which stores up to `n` unique elements from the array `arr`. Each key-value pair in the hash map consumes space, so in the worst case, the space required is proportional to the number of unique elements in `arr`, which is $O(n)$.\n\n    No additional data structures that depend on the size of the input are used, so the auxiliary space is $O(1)$. Therefore, the total space complexity is $O(n)$.\n\n---#### Note on Hash Maps and Sets\n \nHash maps and hash sets are widely used data structures that provide efficient storage and retrieval of data. Their average-case time complexity for most operations, such as insertion, deletion, and lookup, is often $O(1)$. However, this efficiency depends on several factors, and there are edge cases where performance can degrade.\n\n##### Average-Case Complexity: $O(1)$\nHashing Process: The operation relies on a hash function, which maps keys to specific \"buckets\" in memory.\n- A well-designed hash function ensures uniform distribution of keys across buckets. The hash computation itself is expected to be a constant-time operation in most cases.\n- Once the hash is computed, the bucket corresponding to the hash is accessed directly, making the lookup process efficient.\n\n##### Worst-Case Complexity: $O(n)$\nHash Collisions: When multiple keys map to the same bucket due to the hash function returning the same hash code, a collision occurs. \n- In such cases, the hash map stores all colliding entries in a bucket, typically as a linked list.\n- To resolve collisions, the hash map iterates through the bucket, checking each entry with an equality comparison, leading to $O(n)$ time complexity if all keys hash to the same bucket.\n- This worst-case scenario is extremelly rare with a good hash function, but it is still a theoretical limitation to consider when designing or selecting algorithms.\n\n##### Improvements in Modern Implementations (e.g., Java HashMap in JDK 8)\nTree-Backed Buckets: In modern hash map implementations, buckets that become densely populated are converted into balanced binary trees.\n- This reduces the lookup time complexity in such cases from $O(n)$ to $O(\\log n)$. The tree structure leverages key ordering for efficient traversal.\n- If the key type's equality (`equals`) and ordering (`compareTo`) logic are inconsistent, this optimization sometimes leads to unpredictable behavior.\n\n##### Takeaway:\nIn most real-world scenarios, hash maps and sets offer excellent performance and are the default choice for many applications requiring fast lookups.\n\nHowever, be cautious of:\n  - Poor hash functions.\n  - Memory limitations.\n  - Keys with inconsistent equality and ordering logic in modern implementations.\n\nIn general, when analyzing time complexity and space complexity in editorials, videos or discussions on the internet, most assume that hash functions are well-designed. This assumption allows us to consider the complexity of hash table operations as $O(1)$, rather than $O(n)$ or $O(\\log n)$. This consistent behavior is reflected in almost all articles or videos you may encounter, where lookups are always described as constant time, never as $O(n)$ due to the underlying principles of hash functions.\n\nTo deepen your understanding of hash maps, sets, and their underlying principles, the following resources are highly recommended:\n1. [Hash Table Explore Card ðŸ”—](https://leetcode.com/explore/learn/card/hash-table/): This card has some great explanations on how it is applied to different DSA problems and how to recognize this pattern.\n2. [Universal Hashing](https://en.wikipedia.org/wiki/Universal_hashing): This explains the principles behind creating hash functions that achieve the ideal $O(1)$ performance.\n3. [Hash Table](https://en.wikipedia.org/wiki/Hash_table): This covers collision handling techniques (like chaining and open addressing) and practical trade-offs in hash table design.\n\n---"
}